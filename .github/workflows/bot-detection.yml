name: Bot Detection
description: "Detect potential bots by analyzing comment similarity. DOI: https://doi.org/10.1145/3387940.3391503"

on:
  workflow_dispatch:
  schedule:
    - cron: "17 3 * * *"  # daily

permissions:
  contents: read
  pull-requests: read
  issues: write

jobs:
  detect-bots:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install requests

      - name: Run bot detection
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPO: ${{ github.repository }}
        run: |
          python << 'EOF'
          import os, requests, datetime, itertools, math, threading
          from collections import defaultdict
          from concurrent.futures import ThreadPoolExecutor, as_completed

          TOKEN = os.environ["GH_TOKEN"]
          REPO = os.environ["REPO"]
          HEADERS = {"Authorization": f"Bearer {TOKEN}"}

          DAYS_BACK = 3
          MAX_PR = 200
          MIN_MESSAGES = 25
          EPS = 0.22
          BOT_CLUSTER_THRESHOLD = 10
          MAX_WORKERS = 10  # Parallel API requests

          cutoff = datetime.datetime.utcnow() - datetime.timedelta(days=DAYS_BACK)

          def gh(url):
              r = requests.get(url, headers=HEADERS)
              r.raise_for_status()
              return r.json()

          def gh_paginated(url, key=None):
              """Fetch all paginated results from GitHub API"""
              results = []
              page = 1
              while True:
                  paginated_url = f"{url}&page={page}&per_page=100" if "?" in url else f"{url}?page={page}&per_page=100"
                  data = gh(paginated_url)
                  if not data:
                      break
                  results.extend(data)
                  if len(data) < 100:
                      break
                  page += 1
              return results

          # -----------------------------
          # Fetch PRs
          # -----------------------------
          prs = gh(f"https://api.github.com/repos/{REPO}/pulls?state=all&per_page=100&sort=updated&direction=desc")
          user_created_dates = {}  # Cache user account creation dates
          user_cache_lock = threading.Lock()

          comments_by_author = defaultdict(list)
          comment_details = defaultdict(list)  # Track PR/issue links and timestamps
          new_accounts = []  # Track accounts created recently (< 7 days)

          def fetch_pr_comments(pr):
              """Fetch comments for a single PR"""
              try:
                  if datetime.datetime.strptime(pr["updated_at"], "%Y-%m-%dT%H:%M:%SZ") < cutoff:
                      return []
                  
                  number = pr["number"]
                  issue_comments = gh_paginated(f"https://api.github.com/repos/{REPO}/issues/{number}/comments")
                  review_comments = gh_paginated(f"https://api.github.com/repos/{REPO}/pulls/{number}/comments")
                  return [(number, c) for c in (issue_comments + review_comments)]
              except:
                  return []

          # Fetch all comments in parallel
          all_pr_comments = []
          with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
              futures = [executor.submit(fetch_pr_comments, pr) for pr in prs[:MAX_PR]]
              for future in as_completed(futures):
                  all_pr_comments.extend(future.result())

          # Collect unique users that need account age lookup
          users_to_fetch = set()
          for number, c in all_pr_comments:
              created = datetime.datetime.strptime(c["created_at"], "%Y-%m-%dT%H:%M:%SZ")
              if created < cutoff:
                  continue
              user = c.get("user", {})
              login = user.get("login")
              if login:
                  users_to_fetch.add(login)

          # Fetch all user account creation dates in parallel
          def fetch_user_info(login):
              try:
                  user_info = gh(f"https://api.github.com/users/{login}")
                  if user_info:
                      created_at = datetime.datetime.strptime(
                          user_info.get("created_at", ""), "%Y-%m-%dT%H:%M:%SZ"
                      )
                      return (login, created_at)
              except:
                  pass
              return (login, None)

          with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
              futures = [executor.submit(fetch_user_info, login) for login in users_to_fetch]
              for future in as_completed(futures):
                  login, created_at = future.result()
                  user_created_dates[login] = created_at

          # Process comments
          for number, c in all_pr_comments:
              created = datetime.datetime.strptime(c["created_at"], "%Y-%m-%dT%H:%M:%SZ")
              if created < cutoff:
                  continue
              user = c.get("user", {})
              login = user.get("login")
              if not login:
                  continue
              
              # Check if account is suspiciously new (< 7 days old)
              account_created = user_created_dates.get(login)
              if account_created and (cutoff - account_created).days < 7:
                  new_accounts.append((login, (cutoff - account_created).days))
              
              comments_by_author[login].append(c["body"])
              
              # Track comment details: PR#, URL, and timestamp
              comment_url = c.get("html_url", f"https://github.com/{REPO}/pull/{number}#comment")
              comment_details[login].append({
                  "pr": number,
                  "url": comment_url,
                  "timestamp": c["created_at"]
              })

          # -----------------------------
          # Distance metrics
          # -----------------------------
          def levenshtein(a, b):
              if len(a) < len(b):
                  a, b = b, a
              prev = range(len(b) + 1)
              for i, ca in enumerate(a):
                  curr = [i + 1]
                  for j, cb in enumerate(b):
                      insert = prev[j + 1] + 1
                      delete = curr[j] + 1
                      replace = prev[j] + (ca != cb)
                      curr.append(min(insert, delete, replace))
                  prev = curr
              return prev[-1]

          def jaccard(a, b):
              sa = set(a.split())
              sb = set(b.split())
              if not sa and not sb:
                  return 0
              return 1 - len(sa & sb) / len(sa | sb)

          # -----------------------------
          # DBSCAN (precomputed distance)
          # -----------------------------
          def dbscan(dist_matrix, eps):
              n = len(dist_matrix)
              labels = [-1] * n
              cluster_id = 0

              for i in range(n):
                  if labels[i] != -1:
                      continue

                  neighbors = [j for j in range(n) if dist_matrix[i][j] <= eps]

                  if not neighbors:
                      continue

                  labels[i] = cluster_id
                  seeds = neighbors[:]

                  while seeds:
                      j = seeds.pop()
                      if labels[j] == -1:
                          labels[j] = cluster_id
                          new_neighbors = [
                              k for k in range(n)
                              if dist_matrix[j][k] <= eps
                          ]
                          seeds.extend(new_neighbors)

                  cluster_id += 1

              return labels

          # Analyze authors (limit sample size for performance)
          results = []
          suspicious_unknowns = []  # Track accounts that warrant investigation
          MAX_MESSAGES_TO_ANALYZE = 100  # Cap on messages to analyze per author

          for author, messages in comments_by_author.items():
              # Check for new bots: few messages that are all very similar
              if 3 <= len(messages) < MIN_MESSAGES:
                  norm = [m.lower().strip() for m in messages[:MAX_MESSAGES_TO_ANALYZE]]
                  
                  # Quick check: all identical
                  if len(set(norm)) == 1:
                      results.append((author, 1, len(messages)))
                      continue
                  
                  # Check if messages are very similar (likely AI bot)
                  n = len(norm)
                  if n >= 2:
                      # Fast similarity check: sample pairs if too many
                      similarity_sum = 0
                      pair_count = 0
                      pairs = list(itertools.combinations(range(n), 2))
                      sample_pairs = pairs if len(pairs) <= 20 else pairs[::len(pairs)//20]
                      
                      for i, j in sample_pairs:
                          L = levenshtein(norm[i], norm[j]) / max(len(norm[i]), len(norm[j]), 1)
                          J = jaccard(norm[i], norm[j])
                          D = (L + J) / 2
                          similarity_sum += D
                          pair_count += 1
                      
                      # If average distance is very low (< 0.15), messages are nearly identical
                      avg_distance = similarity_sum / pair_count if pair_count > 0 else 0
                      if avg_distance < 0.15:
                          # Cluster into 1 group if all very similar
                          results.append((author, 1, len(messages)))
                      else:
                          # Flag as suspicious unknown: few messages with unusual patterns
                          suspicious_unknowns.append((author, len(messages), avg_distance))
                      continue
              
              if len(messages) < MIN_MESSAGES:
                  continue

              # Limit messages analyzed for large authors
              norm = [m.lower().strip() for m in messages[:MAX_MESSAGES_TO_ANALYZE]]

              n = len(norm)
              dist_matrix = [[0]*n for _ in range(n)]

              # Only compute distance matrix for reasonable numbers of messages
              if n > 50:
                  # For authors with many messages, sample pairs
                  pair_count = 0
                  max_pairs = 500  # Limit pairwise comparisons
                  for i in range(n):
                      for j in range(i+1, min(n, i+10)):  # Compare with nearby messages
                          L = levenshtein(norm[i], norm[j]) / max(len(norm[i]), len(norm[j]))
                          J = jaccard(norm[i], norm[j])
                          D = (L + J) / 2
                          dist_matrix[i][j] = D
                          dist_matrix[j][i] = D
                          pair_count += 1
                          if pair_count >= max_pairs:
                              break
                      if pair_count >= max_pairs:
                          break
              else:
                  # Full matrix for small message counts
                  for i, j in itertools.combinations(range(n), 2):
                      L = levenshtein(norm[i], norm[j]) / max(len(norm[i]), len(norm[j]))
                      J = jaccard(norm[i], norm[j])
                      D = (L + J) / 2
                      dist_matrix[i][j] = D
                      dist_matrix[j][i] = D

              labels = dbscan(dist_matrix, EPS)
              clusters = len(set(labels))

              results.append((author, clusters, len(messages)))

          # -----------------------------
          # Build report - ONLY for high risk accounts
          # -----------------------------
          # Deduplicate new accounts
          high_risk_accounts = {}
          for login, days_old in new_accounts:
              if login not in high_risk_accounts:
                  high_risk_accounts[login] = days_old

          # Only post if we have high-risk accounts
          if not high_risk_accounts:
              print("No high-risk accounts detected. Skipping report.")
          else:
              # Fetch additional activity for high-risk accounts
              high_risk_activity = {}
              
              def fetch_user_activity(login):
                  activity = {
                      "issues": [],
                      "prs": [],
                      "comments": comment_details.get(login, [])
                  }
                  try:
                      # Fetch issues opened by this user
                      issues = gh_paginated(f"https://api.github.com/repos/{REPO}/issues?creator={login}&state=all")
                      activity["issues"] = [{"number": i["number"], "title": i["title"], "created_at": i["created_at"], "html_url": i["html_url"]} for i in issues]
                  except:
                      pass
                  
                  try:
                      # Fetch PRs opened by this user
                      prs = gh_paginated(f"https://api.github.com/repos/{REPO}/pulls?creator={login}&state=all")
                      activity["prs"] = [{"number": p["number"], "title": p["title"], "created_at": p["created_at"], "html_url": p["html_url"]} for p in prs]
                  except:
                      pass
                  
                  return (login, activity)

              with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                  futures = [executor.submit(fetch_user_activity, login) for login in high_risk_accounts.keys()]
                  for future in as_completed(futures):
                      login, activity = future.result()
                      high_risk_activity[login] = activity

              today = datetime.datetime.utcnow().strftime("%Y-%m-%d")
              body = f"# ðŸš¨ HIGH RISK: Brand New Accounts Detected â€” {today}\n\n"
              body += "Recently-created accounts often indicate bots, spam accounts, or coordinate attacks.\n\n"
              
              for login, days_old in sorted(high_risk_accounts.items(), key=lambda x: x[1]):
                  body += f"## @{login}\n"
                  body += f"**Account age:** {days_old} days old\n\n"
                  
                  activity = high_risk_activity.get(login, {})
                  
                  # Issues opened
                  issues = activity.get("issues", [])
                  if issues:
                      body += "### Issues Opened\n"
                      for issue in issues:
                          body += f"- [#{issue['number']}: {issue['title']}]({issue['html_url']}) ({issue['created_at']})\n"
                      body += "\n"
                  
                  # PRs opened
                  prs = activity.get("prs", [])
                  if prs:
                      body += "### Pull Requests Opened\n"
                      for pr in prs:
                          body += f"- [#{pr['number']}: {pr['title']}]({pr['html_url']}) ({pr['created_at']})\n"
                      body += "\n"
                  
                  # Comments
                  comments = activity.get("comments", [])
                  if comments:
                      body += "### Comments\n"
                      for comment in comments[:10]:  # Limit to 10 most recent
                          body += f"- [PR #{comment['pr']}]({comment['url']}) ({comment['timestamp']})\n"
                      if len(comments) > 10:
                          body += f"- ... and {len(comments) - 10} more comments\n"
                      body += "\n"
                  
                  if not issues and not prs and not comments:
                      body += "*(No issues, PRs, or comments found in last {DAYS_BACK} days)*\n\n"

              requests.post(
                  f"https://api.github.com/repos/{REPO}/issues",
                  headers=HEADERS,
                  json={
                      "title": f"ðŸš¨ HIGH RISK: Brand New Accounts â€” {today}",
                      "body": body,
                      "labels": ["security", "bot-detection"]
                  }
              )
          EOF
